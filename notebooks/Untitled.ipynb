{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d978e-eca9-4a78-8781-719bd6adace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to your zip file\n",
    "zip_file_path = 'Raw_time_domian_data.zip'\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('extracted_dataset')  # Extracts to 'extracted_dataset' folder\n",
    "\n",
    "# Path to the extracted directory\n",
    "extracted_path = 'extracted_dataset'\n",
    "# Column names to assign\n",
    "column_names = [\n",
    "    \"time_acc\", \"acc_x\", \"acc_y\", \"acc_z\",\n",
    "    \"time_gyro\", \"gyro_x\", \"gyro_y\", \"gyro_z\"\n",
    "]\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Path to parent directory with extracted data\n",
    "parent_folder_path = 'extracted_dataset/1.Raw_time_domian_data'\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all folders and CSVs\n",
    "for activity_folder in os.listdir(parent_folder_path):\n",
    "    activity_path = os.path.join(parent_folder_path, activity_folder)\n",
    "\n",
    "    # Check if it's a directory (activity folder)\n",
    "    if os.path.isdir(activity_path):\n",
    "        print(f\"📚 Loading data for activity: {activity_folder}\")\n",
    "\n",
    "        # Process each CSV in the activity folder\n",
    "        for file in os.listdir(activity_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(activity_path, file)\n",
    "\n",
    "                try:\n",
    "                    # Read CSV with custom columns and streaming\n",
    "                    df = pl.read_csv(file_path, has_header=False, new_columns=column_names)\n",
    "\n",
    "                    # Add activity and file info for reference\n",
    "                    df = df.with_columns([\n",
    "                        pl.lit(activity_folder).alias('activity'),\n",
    "                        pl.lit(file).alias('source_file')\n",
    "                    ])\n",
    "\n",
    "                    # Append DataFrame to list\n",
    "                    dataframes.append(df)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error loading {file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames if not empty\n",
    "if dataframes:\n",
    "    combined_df = pl.concat(dataframes, how=\"vertical_relaxed\", rechunk=True)\n",
    "    print('✅ All CSVs loaded successfully!')\n",
    "    print(combined_df.head())\n",
    "else:\n",
    "    print('❗ No valid CSV files found!')\n",
    "combined_df.shape\n",
    "# Remove duplicates based on all columns\n",
    "combined_df = combined_df.unique()\n",
    "combined_df.shape\n",
    "# Check for rows with any null values\n",
    "rows_with_nulls = combined_df.filter(combined_df.select(pl.all().is_null()).sum_horizontal() > 0)\n",
    "print(rows_with_nulls)\n",
    "# Drop rows with any null values\n",
    "clean_df = combined_df.drop_nulls()\n",
    "clean_df.shape\n",
    "print(clean_df['activity'].unique())\n",
    "# Remove numbers followed by a dot\n",
    "clean_df = clean_df.with_columns(\n",
    "    pl.col(\"activity\").str.replace(r\"\\d+\\.\\s*\", \"\").alias(\"activity\")\n",
    ")\n",
    "\n",
    "clean_df.shape\n",
    "# Count class occurrences\n",
    "class_counts = clean_df.group_by(\"activity\").agg(\n",
    "    pl.col(\"activity\").count().alias(\"count\")\n",
    ")\n",
    "\n",
    "print(class_counts)\n",
    "# Drop rows where 'activity' equals 'walking'\n",
    "df_filtered = clean_df.filter(clean_df[\"activity\"] != \"Table-tennis\")\n",
    "df_filtered.shape\n",
    "# Convert to Pandas for plotting\n",
    "class_counts_pd = class_counts.to_pandas()\n",
    "\n",
    "# Extract labels and values for plotting\n",
    "labels = class_counts_pd[\"activity\"]\n",
    "counts = class_counts_pd[\"count\"]\n",
    "# Plot the class distribution as a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    counts,\n",
    "    labels=labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    "    colors=plt.cm.Paired.colors\n",
    ")\n",
    "plt.title(\"Class Imbalance in Target Column\")\n",
    "plt.show()\n",
    "# Count occurrences of each activity\n",
    "class_counts = df_filtered.group_by(\"activity\").agg(\n",
    "    pl.col(\"activity\").count().alias(\"count\")\n",
    ")\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "class_counts_pd = class_counts.to_pandas()\n",
    "\n",
    "# Extract labels and counts\n",
    "labels = class_counts_pd[\"activity\"]\n",
    "counts = class_counts_pd[\"count\"]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, counts, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Rotate and align x-axis labels\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Activity Count Distribution\")\n",
    "\n",
    "# Rotate labels and align them to the right\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Automatically adjust subplot to fit the labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# Define the number of records to sample per class (e.g., minimum class size or desired number)\n",
    "min_count = class_counts.select(\"count\").min()[0, 0]\n",
    "print(f\"\\nMinimum records per class for safe sampling: {min_count}\")\n",
    "\n",
    "# Stratified sampling with safe check to avoid errors\n",
    "df_balanced = pl.concat(\n",
    "    [\n",
    "        df_filtered.filter(pl.col(\"activity\") == activity)\n",
    "        .sample(n=min(min_count, df_filtered.filter(pl.col(\"activity\") == activity).height), seed=42)\n",
    "        for activity in df_filtered.select(\"activity\").unique().to_series().to_list()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nStratified Sampled DataFrame with Class Balance:\")\n",
    "print(df_balanced)\n",
    "\n",
    "\n",
    "#further reducing the dataset size\n",
    "# Sample 50% of the dataset\n",
    "df_balanced_sampled = df_filtered.sample(fraction=0.75, seed=42)\n",
    "\n",
    "# Check the size of the new DataFrame\n",
    "print(f\"Original Size: {df_balanced.shape}\")\n",
    "print(f\"Sampled Size: {df_balanced_sampled.shape}\")\n",
    "\n",
    "#further reducing the dataset size\n",
    "# Sample 50% of the dataset\n",
    "df_balanced_sampled = df_balanced.sample(fraction=0.75, seed=42)\n",
    "\n",
    "# Check the size of the new DataFrame\n",
    "print(f\"Original Size: {df_balanced.shape}\")\n",
    "print(f\"Sampled Size: {df_balanced_sampled.shape}\")\n",
    "# Count occurrences of each activity\n",
    "class_counts = df_balanced_sampled.group_by(\"activity\").agg(\n",
    "    pl.col(\"activity\").count().alias(\"count\")\n",
    ")\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "class_counts_pd = class_counts.to_pandas()\n",
    "\n",
    "# Extract labels and counts\n",
    "labels = class_counts_pd[\"activity\"]\n",
    "counts = class_counts_pd[\"count\"]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, counts, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# Rotate and align x-axis labels\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Activity Count Distribution\")\n",
    "\n",
    "# Rotate labels and align them to the right\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Automatically adjust subplot to fit the labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "# Drop specific columns\n",
    "columns_to_drop = [\"activity\", \"source_file\"]\n",
    "df_trimmed = df_balanced_sampled.drop(columns_to_drop)\n",
    "print(df_trimmed)\n",
    "\n",
    "# Check column names\n",
    "print(df_trimmed.columns)\n",
    "# List of columns to scale\n",
    "columns_to_scale = ['time_acc', 'acc_x', 'acc_y', 'acc_z', 'time_gyro', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "\n",
    "\n",
    "# Replace original columns with scaled values\n",
    "# Apply Min-Max Scaling\n",
    "df_scaled_replaced = df_balanced_sampled.with_columns(\n",
    "    [((pl.col(col) - pl.col(col).min()) / (pl.col(col).max() - pl.col(col).min())).alias(col)\n",
    "     for col in columns_to_scale]\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame with Replaced Scaled Columns:\")\n",
    "print(df_scaled_replaced)\n",
    "# Convert Polars DataFrame to Pandas for easier correlation\n",
    "df_pandas = df_trimmed.to_pandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df_pandas.corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix as heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "#time_gyro & time_acc are having very high corelation as can be seen from heatmap , hence we can drop any one of them for the dataset\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df_pandas.corr().abs()\n",
    "\n",
    "# Define correlation threshold\n",
    "correlation_threshold = 0.9\n",
    "\n",
    "# Identify columns to drop based on high correlation\n",
    "to_drop = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if correlation_matrix.iloc[i, j] > correlation_threshold:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            to_drop.add(colname)\n",
    "\n",
    "print(f\"\\nColumns to drop due to high correlation: {to_drop}\")\n",
    "\n",
    "# Drop high-correlation columns in Polars\n",
    "df_scaled_cleaned = df_scaled_replaced.drop(list(to_drop))\n",
    "\n",
    "print(\"\\nDataFrame after dropping highly correlated columns:\")\n",
    "print(df_scaled_cleaned)\n",
    "df_scaled_cleaned=df_scaled_cleaned.drop('source_file')\n",
    "df_scaled_cleaned.head()\n",
    "# Split features and target\n",
    "X = df_scaled_cleaned.select(pl.exclude(\"activity\")).to_numpy()  # Select all columns except \"label\"\n",
    "y = df_scaled_cleaned[\"activity\"].to_numpy()  # Target/label column\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42,stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n",
    "# Create and train the Random Forest model\n",
    "randomforestmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "randomforestmodel.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Model Trained Successfully!\")\n",
    "# Make predictions\n",
    "y_pred = randomforestmodel.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\" Random Forest Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\n Random Forest Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "#!pip install xgboost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode target labels (Convert categorical to numeric)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix for XGBoost (efficient training format)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multiclass classification\n",
    "    'num_class': len(set(y)),  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Log loss for multiclass classification\n",
    "    'max_depth': 6,  # Maximum depth of a tree\n",
    "    'eta': 0.05,  # Learning rate\n",
    "    'gamma': 0.1,  # Minimum loss reduction required for split\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_rounds = 500  # Number of boosting rounds\n",
    "xgbmodel = xgb.train(params, dtrain, num_boost_round=num_rounds)\n",
    "print(\"XGBoost Model Trained Successfully!\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgbmodel.predict(dtest)\n",
    "\n",
    "# Evaluate model \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
